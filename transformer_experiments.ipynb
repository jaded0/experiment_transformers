{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Experiments\n",
    "I was thinking about how the attention layer in transformers literally just uses linear layers with a couple dot product operations. I wondered if such a process was learnable by a simple neural network, but in my brainstorm sesh with ChatGPT, it didn't like the idea. In the 'Attention is all you need' paper, they talk about how attention sort of reduces the path between dependencies in the input. Like, it allows the input to compare itself *to itself*. So, it's hard to replicate that with linear layers because in order to do so requires a significant amount of depth. Anyways, the key idea here is to make a sort of dot product layer with learnable parameters so that the model has more immediate access (without so much depth required) to compare a set of activations to itself. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "This is GPT-2, hopefully modifiable so that I can swap out its attention. If not, that's just embarrassing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm experimenting with a new architecture for a transformer. I only have my rtx 3090 laptop gpu, for now, which has 16gb memory. I'd like to be able to train a transformer, with passable results, just on my machine, so I need a baseline model and dataset which are small enough in resources for me to be able to iterate fairly quickly as I tinker with the model structure. I'd like it to be a decoder-only model, as well. \n",
    "Given your constraints, you can start by working with a small-scale decoder-only transformer model and a modestly-sized dataset. Here's a suggestion for the baseline model and dataset:\n",
    "\n",
    "Baseline Model:\n",
    "Since you want a decoder-only model, you can use a scaled-down version of GPT-2 as your baseline. The GPT-2 architecture is based on the Transformer architecture and is designed for text generation tasks. To fit your GPU memory, you can reduce the number of layers (transformer blocks) and the hidden size of the model, as well as the number of attention heads.\n",
    "\n",
    "For example, instead of using the original GPT-2 configuration with 12 layers, 768 hidden units, and 12 attention heads, you could experiment with a smaller model, such as 4 layers, 256 hidden units, and 4 attention heads.\n",
    "\n",
    "Dataset:\n",
    "A suitable dataset would be small enough to fit in your GPU memory and enable quick experimentation but still diverse enough to allow your model to learn meaningful representations. One possible option is the \"WikiText-2\" dataset, a collection of Wikipedia articles containing over 2 million tokens. This dataset is widely used for language modeling tasks and is smaller than the \"WikiText-103\" dataset.\n",
    "\n",
    "Training:\n",
    "To accelerate training, you can use mixed-precision training, which leverages half-precision (float16) arithmetic to reduce memory usage and training time. Additionally, you can experiment with gradient accumulation to further fit your model within the GPU memory constraints.\n",
    "\n",
    "Remember to start with a smaller batch size and lower learning rate to accommodate your GPU's memory limitations, and gradually increase them if possible. Monitor the training loss and performance on a validation set to ensure that the model is learning effectively.\n",
    "\n",
    "Once you have established a baseline with the scaled-down GPT-2 model and WikiText-2 dataset, you can start experimenting with your new architecture and compare its performance against the baseline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from torch.optim import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiText2Dataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, seq_length):\n",
    "        with open(data_path, \"r\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Split the text into smaller chunks and tokenize them\n",
    "        chunk_size = seq_length * 10\n",
    "        text_chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        tokenized_chunks = []\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            tokenized_chunk = tokenizer(chunk, return_tensors=\"pt\", add_special_tokens=True)[\"input_ids\"]\n",
    "            if tokenized_chunk.size(1) < seq_length:\n",
    "                tokenized_chunk = torch.cat([tokenized_chunk, torch.zeros(1, seq_length - tokenized_chunk.size(1), dtype=torch.long)], dim=1)\n",
    "            tokenized_chunks.append(tokenized_chunk)\n",
    "\n",
    "        # Create input sequences with the specified sequence length\n",
    "        self.inputs = []\n",
    "\n",
    "        for tokenized_chunk in tokenized_chunks:\n",
    "            for i in range(0, tokenized_chunk.size(1) - seq_length, seq_length):\n",
    "                input_sequence = tokenized_chunk[0, i:i + seq_length]\n",
    "                self.inputs.append(input_sequence)\n",
    "\n",
    "        self.inputs = torch.stack(self.inputs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <|endoftext|>, pad_token_id: 50256\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and the dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(f'pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}')\n",
    "seq_length = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "train_dataset = WikiText2Dataset(\"wikitext-2-raw/wiki.train.raw\", tokenizer, seq_length)\n",
    "\n",
    "data_on_gpu = False\n",
    "if data_on_gpu:\n",
    "    # Convert the entire dataset to a tensor and move it to the GPU\n",
    "    print('Converting the entire dataset to a tensor and moving it to the GPU')\n",
    "    train_dataset = torch.stack([sample.to(device) for sample in train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 62302\n",
      "Number of batches: 7788\n",
      "train_loader length: 7788\n",
      "Warmup fraction: 0.0012840267077555213\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of batches:\", len(train_loader))\n",
    "\n",
    "# Define the model configuration and instantiate the GPT-2 model\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=tokenizer.max_model_input_sizes[\"gpt2\"],\n",
    "    n_ctx=tokenizer.max_model_input_sizes[\"gpt2\"],\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    activation_function=\"gelu\"\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate = 2e-4\n",
    "warmup_steps = 50  # Reduce the warmup steps to ensure the warmup_fraction is within the expected range.\n",
    "\n",
    "def setup():\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    print(f'train_loader length: {len(train_loader)}')\n",
    "    warmup_fraction = warmup_steps / (len(train_loader) * num_epochs)\n",
    "    print(\"Warmup fraction:\", warmup_fraction)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, total_steps=len(train_loader) * num_epochs, pct_start=warmup_fraction)\n",
    "    return optimizer, scheduler\n",
    "optimizer,scheduler = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_loss_graph(losses, epoch):\n",
    "    clear_output(wait=True)  # Clear the output of the current cell\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Loss graph at epoch {epoch}\")\n",
    "    plt.show()\n",
    "\n",
    "def generate_sample(model, tokenizer, prompt, max_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    # attention_mask = (input_ids != tokenizer.pad_token_id).to(device)  # Create the attention mask\n",
    "    with torch.no_grad():\n",
    "        # output_ids = model.generate(input_ids, max_length=max_length, attention_mask=attention_mask)  # Pass the attention mask to the model\n",
    "        output_ids = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def train(model, optimizer, scheduler):\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids = batch.to(device)\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        plot_loss_graph(losses, epoch + 1)\n",
    "\n",
    "        # Switch to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Generate a sample inference\n",
    "        prompt = \"In a shocking turn of events,\"\n",
    "        generated_text = generate_sample(model, tokenizer, prompt)\n",
    "        print(f\"Generated text at epoch {epoch + 1}: {generated_text}\")\n",
    "\n",
    "        # Switch back to training mode\n",
    "        model.train()\n",
    "\n",
    "\n",
    "    # Plot the final loss graph\n",
    "    plot_loss_graph(losses, 'Final')\n",
    "\n",
    "\n",
    "    # Generate a sample inference\n",
    "    model.eval()\n",
    "    prompt = \"In a shocking turn of events,\"\n",
    "    generated_text = generate_sample(model, tokenizer, prompt)\n",
    "    print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# og_losses = train(model)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)     \n",
    "     \n",
    "Generated text: In a shocking turn of events,@ 000 people, the player can be used to be a player. \n",
    " \n",
    " = = = = = = = = = = = = = = = = = = = = = = = = ="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Time!\n",
    "I've created a Dot Product Interaction Layer, which aims to provide that functionality to compare one set of activations with itself or others directly. Attention seems like it does that. So, I hope that attention emerges from the usage of this layer. I'm going to try to use it in the transformer used above and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've created a Dot Product Interaction Layer, which aims to provide that functionality to compare one set of activations with itself or others directly. Attention seems like it does that. So, I hope that attention emerges from the usage of this layer. I'm going to try to use it in the transformer used above and see what happens.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DotProductInteractionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom layer that computes the dot product interaction between two input tensors\n",
    "    using a learned interaction matrix. The interaction matrix is thresholded to \n",
    "    control the amount of interaction between elements in the input tensors.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): The size of the input tensor's last dimension.\n",
    "        threshold (float, optional): The threshold value for the interaction matrix.\n",
    "                                     Elements with an absolute value below the threshold\n",
    "                                     will be set to zero. Default is 0.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, n_head, threshold=0.0):\n",
    "        super(DotProductInteractionLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_head = n_head\n",
    "        self.threshold = threshold\n",
    "        self.interaction_matrix = nn.Parameter(torch.randn(input_dim // n_head, input_dim // n_head))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Forward pass of the DotProductInteractionLayer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The first input tensor of shape (batch_size, sequence_length, input_dim).\n",
    "            y (torch.Tensor): The second input tensor of shape (batch_size, sequence_length, input_dim).\n",
    "            \n",
    "        Returns:\n",
    "            interaction_output (torch.Tensor): Output tensor of shape (batch_size, sequence_length, input_dim),\n",
    "                                               representing the dot product interaction between the input tensors.\n",
    "        \"\"\"\n",
    "        print(f'x shape: {x.shape}, y shape: {y.shape}')\n",
    "        # Compute the absolute interaction matrix\n",
    "        abs_interaction_matrix = torch.abs(self.interaction_matrix)\n",
    "        print(f'abs_interaction_matrix shape: {abs_interaction_matrix.shape}')\n",
    "\n",
    "        # Apply thresholding to the interaction matrix\n",
    "        threshold_mask = (abs_interaction_matrix > self.threshold).float()\n",
    "        thresholded_interaction_matrix = abs_interaction_matrix * threshold_mask\n",
    "        print(f'thresholded_interaction_matrix shape: {thresholded_interaction_matrix.shape}')\n",
    "\n",
    "        # Compute the dot product between the input activations and the thresholded interaction matrix\n",
    "        interaction_result = torch.matmul(x, thresholded_interaction_matrix)\n",
    "        print(f'interaction_result shape before final matmul: {interaction_result.shape}')\n",
    "\n",
    "        # Compute the dot product between the interaction result and the second input tensor (y)\n",
    "        interaction_output = torch.matmul(interaction_result, y.transpose(-1, -2))\n",
    "        \n",
    "        return interaction_output\n",
    "    \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "\n",
    "class CustomDotProductAttention(GPT2Attention):\n",
    "    \"\"\"\n",
    "    Custom attention module that replaces the original self-attention mechanism in GPT-2 with a Dot Product Interaction Layer.\n",
    "    \n",
    "    Args:\n",
    "        config (:obj:`transformers.GPT2Config`): Model configuration class with all the parameters of the model.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dk = config.n_embd // self.n_head\n",
    "        self.interaction_layer = DotProductInteractionLayer(config.n_embd, config.n_head, threshold=config.attn_threshold)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        layer_past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass of the custom dot product attention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_dim).\n",
    "            layer_past (Optional[torch.Tensor]): Cached past layer for faster decoding. Default is None. Unused in this implementation.\n",
    "            attention_mask (Optional[torch.Tensor]): Mask to apply on the attention outputs. Default is None.\n",
    "            head_mask (Optional[torch.Tensor]): Mask to apply on the attention heads. Default is None.\n",
    "            encoder_hidden_states (Optional[torch.Tensor]): Encoder hidden states tensor. Default is None. Unused in this implementation.\n",
    "            encoder_attention_mask (Optional[torch.Tensor]): Mask to apply on the encoder attention outputs. Default is None. Unused in this implementation.\n",
    "            use_cache (bool): Whether or not to use cached layer. Default is False. Unused in this implementation.\n",
    "            output_attentions (bool): Whether or not to output attention weights. Default is False. Unused in this implementation.\n",
    "            \n",
    "        Returns:\n",
    "            attn_out (torch.Tensor): Output tensor of shape (batch_size, sequence_length, input_dim).\n",
    "        \"\"\"\n",
    "    \n",
    "        # Transform input activations into keys, queries, and values\n",
    "        keys = self.c_attn(x).split(self.split_size, dim=2)\n",
    "        queries, keys, values = map(lambda t: t.view(*t.size()[:-1], self.n_head, self.dk).transpose(1, 2), keys)\n",
    "\n",
    "        print(f'queries: {queries.shape}, keys: {keys.shape}, values: {values.shape}')\n",
    "        # Compute the dot product between the keys and queries using the interaction layer\n",
    "        interaction_result = self.interaction_layer(queries.reshape(-1, self.dk), keys.transpose(-1, -2).reshape(-1, self.dk))\n",
    "\n",
    "        print(f'interaction_result: {interaction_result.shape}')\n",
    "        # Reshape the interaction_result to the original shape\n",
    "        interaction_result = interaction_result.reshape(queries.size(0), queries.size(1), self.n_head, -1).transpose(1, 2)\n",
    "        print(f'interaction_result after reshaping: {interaction_result.shape}')\n",
    "        # Apply the scaling factor\n",
    "        scaling_factor = self.dk ** 0.5\n",
    "        interaction_logits = interaction_result / scaling_factor\n",
    "\n",
    "        print(f'interaction_logits, before softmax: {interaction_logits.shape}')\n",
    "        # Normalize the attention logits using the softmax function\n",
    "        attention_weights = F.softmax(interaction_logits, dim=-1)\n",
    "        print(f'attention_weights after softmax: {attention_weights.shape}')\n",
    "        # # Apply the attention mask if provided\n",
    "        # if attention_mask is not None:\n",
    "        #     attention_weights = attention_weights * attention_mask.unsqueeze(1).unsqueeze(2).to(attention_weights.dtype)\n",
    "\n",
    "        # # Apply the head mask if provided\n",
    "        # if head_mask is not None:\n",
    "        #     attention_weights = attention_weights * head_mask.unsqueeze(1).unsqueeze(2).unsqueeze(3).to(attention_weights.dtype)\n",
    "\n",
    "        # # Compute the weighted sum of the value representations\n",
    "        # print(f'attention_weights: {attention_weights.shape}, values: {values.shape}')\n",
    "        # weighted_values = torch.matmul(attention_weights, values)\n",
    "\n",
    "        # Combine the weighted values with the original input\n",
    "        # attn_out = self.c_proj(weighted_values.transpose(1, 2).contiguous().view(*weighted_values.size()[:-2], self.n_embd))\n",
    "\n",
    "        attn_out = self.c_proj(attention_weights.transpose(1, 2).contiguous().view(*attention_weights.size()[:-2], self.n_embd))\n",
    "        return attn_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# input_dim = 10\n",
    "# batch_size = 5\n",
    "# threshold = 0.5\n",
    "# x = torch.randn(batch_size, input_dim)\n",
    "# layer = DotProductInteractionLayer(input_dim, threshold)\n",
    "# output = layer(x)\n",
    "\n",
    "# print(output.shape)  # Output should have the same shape as input: (batch_size, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "\n",
    "\n",
    "# class CustomGPT2Attention(GPT2Attention):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "\n",
    "#         # Replace the self-attention mechanism with the DotProductInteractionLayer\n",
    "#         self.attn = DotProductInteractionLayer(config.n_embd, threshold=config.attn_threshold)\n",
    "\n",
    "#     def forward(self, x, layer_past=None, attention_mask=None, head_mask=None):\n",
    "#         # Pass the inputs through the DotProductInteractionLayer\n",
    "#         x = self.attn(x)\n",
    "\n",
    "#         # Continue with the original forward method\n",
    "#         outputs = super().forward(x, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask)\n",
    "#         return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n",
    "\n",
    "class CustomGPT2Model(GPT2Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Replace the GPT2Attention layer with the CustomDotProductAttention layer\n",
    "        self.h = nn.ModuleList([CustomDotProductAttention(config) for _ in range(config.n_layer)])\n",
    "\n",
    "        # # Replace the GPT2Attention layer with the CustomGPT2Attention layer\n",
    "        # self.attention = CustomGPT2Attention(config)\n",
    "\n",
    "\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n",
    "\n",
    "class CustomGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Replace the GPT2Model with the CustomGPT2Model\n",
    "        self.transformer = CustomGPT2Model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomGPT2LMHeadModel(\n",
       "  (transformer): CustomGPT2Model(\n",
       "    (wte): Embedding(50257, 256)\n",
       "    (wpe): Embedding(1024, 256)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): CustomDotProductAttention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (interaction_layer): DotProductInteractionLayer()\n",
       "      )\n",
       "      (1): CustomDotProductAttention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (interaction_layer): DotProductInteractionLayer()\n",
       "      )\n",
       "      (2): CustomDotProductAttention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (interaction_layer): DotProductInteractionLayer()\n",
       "      )\n",
       "      (3): CustomDotProductAttention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (interaction_layer): DotProductInteractionLayer()\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=tokenizer.max_model_input_sizes[\"gpt2\"],\n",
    "    n_ctx=tokenizer.max_model_input_sizes[\"gpt2\"],\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    activation_function=\"gelu\",\n",
    "    attn_threshold=0.5  # Add this line\n",
    ")\n",
    "\n",
    "custom_model = CustomGPT2LMHeadModel(config)\n",
    "custom_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader length: 7788\n",
      "Warmup fraction: 0.0012840267077555213\n"
     ]
    }
   ],
   "source": [
    "# model = custom_model\n",
    "optimizer, scheduler = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries: torch.Size([8, 4, 32, 64]), keys: torch.Size([8, 4, 32, 64]), values: torch.Size([8, 4, 32, 64])\n",
      "x shape: torch.Size([1024, 64]), y shape: torch.Size([1024, 64])\n",
      "abs_interaction_matrix shape: torch.Size([64, 64])\n",
      "thresholded_interaction_matrix shape: torch.Size([64, 64])\n",
      "interaction_result shape before final matmul: torch.Size([1024, 64])\n",
      "interaction_result: torch.Size([1024, 1024])\n",
      "interaction_result after reshaping: torch.Size([8, 4, 4, 8192])\n",
      "interaction_logits, before softmax: torch.Size([8, 4, 4, 8192])\n",
      "attention_weights after softmax: torch.Size([8, 4, 4, 8192])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, 4, 256]' is invalid for input of size 1048576",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(custom_model, optimizer, scheduler)\n",
      "Cell \u001b[1;32mIn[184], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     28\u001b[0m     input_ids \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 29\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids, labels\u001b[39m=\u001b[39;49minput_ids)\n\u001b[0;32m     30\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m     32\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1075\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1075\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   1076\u001b[0m     input_ids,\n\u001b[0;32m   1077\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1078\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1079\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1080\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1081\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1082\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1083\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1084\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1085\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1086\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1087\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1088\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1089\u001b[0m )\n\u001b[0;32m   1090\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1092\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:899\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    889\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    890\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    891\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    896\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    897\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    900\u001b[0m         hidden_states,\n\u001b[0;32m    901\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    902\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    903\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    904\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    905\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    906\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    907\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    908\u001b[0m     )\n\u001b[0;32m    910\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    911\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[186], line 139\u001b[0m, in \u001b[0;36mCustomDotProductAttention.forward\u001b[1;34m(self, x, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mattention_weights after softmax: \u001b[39m\u001b[39m{\u001b[39;00mattention_weights\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[39m# # Apply the attention mask if provided\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39m# if attention_mask is not None:\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[39m#     attention_weights = attention_weights * attention_mask.unsqueeze(1).unsqueeze(2).to(attention_weights.dtype)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39m# Combine the weighted values with the original input\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m# attn_out = self.c_proj(weighted_values.transpose(1, 2).contiguous().view(*weighted_values.size()[:-2], self.n_embd))\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attention_weights\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39;49mview(\u001b[39m*\u001b[39;49mattention_weights\u001b[39m.\u001b[39;49msize()[:\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_embd))\n\u001b[0;32m    140\u001b[0m \u001b[39mreturn\u001b[39;00m attn_out\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[8, 4, 256]' is invalid for input of size 1048576"
     ]
    }
   ],
   "source": [
    "train(custom_model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
